
/*
 * Imports the specified URL as training data by scraping the URL,
 * including all internal hyperlinks, and generate training data
 * from content found.
 */
.arguments
   url:string
   type:string
   delay:int
   max:int
   threshold:int

// Ensures user is authorized to access endpoint.
auth.ticket.verify:root

// Sanity checking invocation.
validators.mandatory:x:@.arguments/*/url
validators.mandatory:x:@.arguments/*/threshold
validators.url:x:@.arguments/*/url
validators.mandatory:x:@.arguments/*/type
validators.mandatory:x:@.arguments/*/delay
validators.integer:x:@.arguments/*/delay
   min:500
   max:30000
validators.mandatory:x:@.arguments/*/max
validators.integer:x:@.arguments/*/max
   min:1
   max:5000
validators.integer:x:@.arguments/*/threshold
   min:25

// Doing some basic logging.
log.info:Crawling URL generating OpenAI training snippets
   url:x:@.arguments/*/url
   type:x:@.arguments/*/type
   delay:x:@.arguments/*/delay
   max:x:@.arguments/*/max
   threshold:x:@.arguments/*/threshold

// Creating a thread and invoking file doing the heavy lifting.
insert-before:x:./*/fork/0
   get-nodes:x:@.arguments
fork

   // Making sure exceptions does not leave thread.
   try

      // Checking if website has a sitemap.
      strings.concat
         get-value:x:@.arguments/*/url
         .:"/sitemap.xml"
      http.get:x:@strings.concat
         headers
            User-Agent:Aista-MachineLearning-Spider
      if
         and
            mte:x:@http.get
               .:int:200
            lt:x:@http.get
               .:int:300
         .lambda

            // Site has a sitemap, ignoring HTML and processing sitemap.xml instead.
            log.info:Site contains a sitemap
               url:x:@.arguments/*/url

            // Converting sitemap to XML.
            xml2lambda:x:@http.get/*/content

            // Used as buffer for URLs.
            .urls

            // Iterating through each URL in currently primary sitemap file.
            for-each:x:@xml2lambda/*/urlset/*/url/*/loc/*/\#text
               unwrap:x:+/*/*
               add:x:@.urls
                  .
                     .:x:@.dp/#

            // Iterating through each CDATA URL in currently primary sitemap file.
            for-each:x:@xml2lambda/*/urlset/*/url/*/loc/*/\#cdata-section
               unwrap:x:+/*/*
               add:x:@.urls
                  .
                     .:x:@.dp/#

            // Iterating through each URL referenced in main sitemap.
            for-each:x:@xml2lambda/*/sitemapindex/*/sitemap/*/loc/*/\#text

               // Getting XML from currently iterated sitemap subset.
               unwrap:x:+/*/*
               http.get:x:@.dp/#
                  headers
                     User-Agent:Aista-MachineLearning-Spider

               // Verifying above invocation returned success.
               if
                  and
                     mte:x:@http.get
                        .:int:200
                     lt:x:@http.get
                        .:int:300
                  .lambda

                     // Above invocation returned success.
                     xml2lambda:x:@http.get/*/content

                     // Iterating through each URL in currently iterated sitemap file.
                     for-each:x:@xml2lambda/*/urlset/*/url/*/loc/*/\#text
                        unwrap:x:+/*/*
                        add:x:@.urls
                           .
                              .:x:@.dp/#

               else

                  // Doing some logging.
                  log.error:Bogus sitemap reference
                     url:x:@.dp/#
                     primary-url:x:@.arguments/*/url

            // Iterating through each CDATA URL referenced in main sitemap.
            for-each:x:@xml2lambda/*/sitemapindex/*/sitemap/*/loc/*/\#cdata-section

               // Getting XML from currently iterated sitemap subset.
               unwrap:x:+/*/*
               http.get:x:@.dp/#
                  headers
                     User-Agent:Aista-MachineLearning-Spider

               // Verifying above invocation returned success.
               if
                  and
                     mte:x:@http.get
                        .:int:200
                     lt:x:@http.get
                        .:int:300
                  .lambda

                     // Above invocation returned success.
                     xml2lambda:x:@http.get/*/content

                     // Iterating through each URL in currently iterated sitemap file.
                     for-each:x:@xml2lambda/*/urlset/*/url/*/loc/*/\#cdata-section
                        unwrap:x:+/*/*
                        add:x:@.urls
                           .
                              .:x:@.dp/#

               else

                  // Doing some logging.
                  log.error:Bogus sitemap reference
                     url:x:@.dp/#
                     primary-url:x:@.arguments/*/url

            // Doing some logging.
            get-count:x:@.urls/*
            log.info:URLs found from sitemap(s)
               url:x:@.arguments/*/url
               urls:x:@get-count

            // Looping through each URL found from sitemap(s).
            .no:int:0
            for-each:x:@.urls/*

               // Checking if we've reached limit.
               if
                  lte:x:@.no
                     get-value:x:@.arguments/*/max
                  .lambda

                     /*
                      * Invoking crawl implementation file, now with explicit list of links,
                      * informing invocation that we do NOT want to process links.
                      */
                     add:x:./*/io.file.execute
                        get-nodes:x:@.arguments/*/type
                        get-nodes:x:@.arguments/*/delay
                        get-nodes:x:@.arguments/*/max
                        get-nodes:x:@.arguments/*/threshold
                     unwrap:x:+/*
                     io.file.execute:/system/openai/crawl.implementation/import-url.hl
                        url:x:@.dp/#
                        crawl:bool:false

                     // Incrementing count of URLs we've processed.
                     math.increment:x:@.no

      else
      
         // Site does not have a sitemap.xml, processing HTML instead.
         log.info:Site does not have a sitemap
            url:x:@.arguments/*/url
         add:x:./*/io.file.execute
            get-nodes:x:@.arguments/*
         io.file.execute:/system/openai/crawl.implementation/import-url.hl
            crawl:bool:true

      /*
       * Crawling is done.
       * Making sure we notify client that we're done and do some logging.
       */
      sockets.signal:magic.backend.message
         roles:root
         args
            message:Done creating OpenAI training data from URL
            type:success

      // Basic logging.
      log.info:OpenAI training data successfully created
         url:x:@.arguments/*/url
         type:x:@.arguments/*/type

   .catch

      // Oops ...!!
      log.error:x:@.arguments/*/message
         url:x:@.arguments/*/url

// Returning success to caller.
return
   result:success
